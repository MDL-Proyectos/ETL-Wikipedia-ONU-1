# ETL-Wikipedia-ONU-1
Normalizaci√≥n y almacenamiento de datos provenientes del proyecto de web scraping.
# Proyecto ETL: Estados Miembros de la ONU

Este repositorio corresponde a la **segunda etapa** del proyecto sobre los Estados miembros de la ONU. Aqu√≠ se realiza el proceso de **ETL (Extract, Transform, Load)**, tomando como fuente los datos extra√≠dos mediante web scraping en la primera parte del proyecto ([ver repositorio WebScraping--0](https://github.com/MDL-Proyectos/WebScraping-proyecto-1.git)).

## üåê Objetivo

Procesar, limpiar y almacenar en una base de datos PostgreSQL la informaci√≥n estructurada sobre los pa√≠ses miembros de la ONU, previamente obtenida desde Wikipedia. El objetivo final es contar con datos normalizados y listos para an√°lisis y visualizaci√≥n.

## üõ†Ô∏è Tecnolog√≠as utilizadas

- Python 3
- psycopg2 (conexi√≥n a PostgreSQL)
- python-dotenv (manejo de variables de entorno)
- pandas (opcional para an√°lisis)
- PostgreSQL

## üì¶ Instalaci√≥n

1. Clona el repositorio.
2. Instala las dependencias necesarias:

```bash
pip install psycopg2-binary python-dotenv pandas
```

3. Configura el archivo `.env` con tus credenciales de base de datos y la ruta al archivo JSON generado en la etapa de scraping:

```env
DB_HOST=
DB_DATABASE=
DB_PORT=
DB_USER= tu_usuario
DB_PSS= tu_contrase√±a

DATA_JSON_PATH= \paises-2.json
```

## ‚ñ∂Ô∏è Ejecuci√≥n

Ejecuta el script principal para iniciar el proceso ETL:

```bash
python main.py
```

El script realiza las siguientes etapas:
1. **Extract:** Lee el archivo JSON generado en la etapa de scraping.
2. **Transform:** Limpia y normaliza los datos (corrige nombres, fechas, elimina caracteres extra, etc.).
3. **Load:** Inserta los datos en la base de datos PostgreSQL en la tabla `paises_onu`.

## ‚öôÔ∏è Estructura del repositorio

```
‚îú‚îÄ‚îÄ main.py                  # Script principal del proceso ETL
‚îú‚îÄ‚îÄ extract_data.py          # M√≥dulo de extracci√≥n de datos
‚îú‚îÄ‚îÄ transform_data.py        # M√≥dulo de transformaci√≥n y normalizaci√≥n
‚îú‚îÄ‚îÄ connection_db.py         # M√≥dulo de conexi√≥n y carga en PostgreSQL
‚îú‚îÄ‚îÄ .env                     # Variables de entorno (no subir credenciales reales)
‚îú‚îÄ‚îÄ README.md                # Documentaci√≥n del proyecto
```

## üîó Relaci√≥n con la primera etapa

Este proyecto depende del archivo JSON generado en el repositorio [WebScraping--0](../WebScraping--0), donde se realiza el scraping de Wikipedia para obtener los datos originales de los pa√≠ses miembros de la ONU.

## ‚öñÔ∏è Consideraciones √©ticas

- Los datos procesados provienen de contenido p√∫blico de Wikipedia.
- Se respeta la privacidad y las buenas pr√°cticas de manejo de datos.

## ‚úçÔ∏è Autor

Matias DL

---
